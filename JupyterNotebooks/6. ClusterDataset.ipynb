{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster drawing to create samples from it\n",
    "Previous attemps to learn on entire drawing was unsuccessfull, so I decided to move to new approach.\n",
    "I.e. i will try to create a cluster of lines from the drawing that will be a learning sample x.\n",
    "Than we will calculate some distance from cluster to dimensions on the drawing to attribute each dimension with the cluster. That attribution will be our Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with basic example of unsupervised clustering with unknown number of clusters\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "\n",
    "# from processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('mongodb://10.0.4.26:27017')\n",
    "db = client.geometry\n",
    "\n",
    "fileids  = db.files.find({'Valid':True}).distinct('FileId')\n",
    "print('files count', len(fileids))\n",
    "\n",
    "# all the data will be too much for now, so we will take say 42 files:\n",
    "fileids = fileids[0:300]\n",
    "\n",
    "query = {\n",
    "    'ClassName':\n",
    "    {\n",
    "        '$in':[\n",
    "            'AcDbLine',\n",
    "            #'AcDbPolyline',\n",
    "            'AcDbText',\n",
    "            'AcDbRotatedDimension'\n",
    "        ]\n",
    "    },\n",
    "    'FileId':{\n",
    "        '$in':fileids\n",
    "    }\n",
    "}\n",
    "\n",
    "data = pd.DataFrame(list(db.objects.find(query)))\n",
    "print('data length', len(data))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What am I going to do here:\n",
    "We group queried data by FileId\n",
    "We get points from each file's primitives (including dimensions). We save each object in dictionary with its points.\n",
    "We cluster points (get clusters from each file)\n",
    "We get objects in each cluster \n",
    "So a single sample is single a X: tensor shape(1..nobjects) with features of objects, Y: tensor shape ndims(could be 0) of with features of dimensions \n",
    "\n",
    "Than in inference we're going to start command that will take entire drawing's entities, cluster them and predict dimensions for each cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data coordinates to columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing import Col2Numpy\n",
    "\n",
    "groupped = data.groupby(['FileId'])\n",
    "column_names = ['StartPoint', 'EndPoint', 'XLine1Point', 'XLine2Point']\n",
    "\n",
    "# Whole point here is that we're keeping index from the original dataset in file_points\n",
    "\n",
    "file_points = {} #dic[FileId]list_Of_Points\n",
    "# all file ids\n",
    "file_ids = list(groupped.groups.keys())\n",
    "for file_id in file_ids:\n",
    "    pnts = Col2Numpy(groupped.get_group(file_id), column_names)\n",
    "    # print(pnts)\n",
    "    file_points[file_id]=pnts\n",
    "\n",
    "num_points = 0\n",
    "for k in file_points:\n",
    "    num_points += len(file_points[k])\n",
    "print('Total number of points: ', num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement and check clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clusterize(x, eps = 0.2):\n",
    "    '''\n",
    "    The very basic clustering with unknown number of clusters\n",
    "    https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py\n",
    "    '''\n",
    "    \n",
    "    # algorithm='kd_tree'\n",
    "    db = DBSCAN(eps=eps, min_samples=6, n_jobs=-1).fit(x)\n",
    "    labels = db.labels_\n",
    "    \n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "    \n",
    "    return labels, n_clusters, n_noise, db.core_sample_indices_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and display clustering result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_graphics import draw_set\n",
    "from processing import scale_ds\n",
    "\n",
    "\n",
    "def AddLabels2Dataset(points_of_one_file, df_to_export_labels, fileid_to_print=\"\", epses=0.2):\n",
    "    result = df_to_export_labels\n",
    "    \n",
    "    # split indexes from coordinates\n",
    "    x1, data_indexes = np.split(points_of_one_file,[3], axis = 1)\n",
    "    data_indexes = data_indexes.astype('int')\n",
    "    \n",
    "    # print(x1)\n",
    "    # scale dataframe in order to clusterize it properly\n",
    "    x1, scl = scale_ds(x1)\n",
    "    \n",
    "    # for now we will magically assign eps on base of scale\n",
    "    # epses = 15*scl\n",
    "    \n",
    "    # epses could be a number or list of numbers\n",
    "    # I use list of numbers in order\n",
    "    # to visually assess clustering\n",
    "    ep = epses\n",
    "    if type(epses) != list:\n",
    "        ep=[epses]\n",
    "        \n",
    "    for eps in ep:\n",
    "        # perform clustering of points to calculate labels\n",
    "        \n",
    "        labels, n_clusters, n_noise, core_indices = Clusterize(x1, eps=eps)\n",
    "\n",
    "        # print(len(x1), len(labels))\n",
    "        # draw_set(x1, labels, core_indices)  \n",
    "\n",
    "        print(\"fileid:{} points:{} clusters:{} noise pnts:{} eps:{:0.3f} scale:{:0.6f}\".format(\n",
    "              fileid_to_print,\n",
    "              x1.shape[0], \n",
    "              n_clusters,\n",
    "              n_noise, \n",
    "              eps,\n",
    "              scl))\n",
    "    \n",
    "    data_indexes['label'] = labels\n",
    "\n",
    "    # https://stackoverflow.com/questions/22918212/fastest-way-to-drop-duplicated-index-in-a-pandas-dataframe\n",
    "    # data_indexes = data_indexes.drop_duplicates('id')\n",
    "    data_indexes = data_indexes.groupby(data_indexes.index).first()\n",
    "    \n",
    "    for i in range(len(data_indexes.index)):\n",
    "        indx = data_indexes.index[i]\n",
    "        # https://stackoverflow.com/questions/13021654/get-column-index-from-column-name-in-python-pandas\n",
    "        result.iloc[indx, result.columns.get_loc('label')] = data_indexes.iloc[i, data_indexes.columns.get_loc('label')]    \n",
    "    print(len(result.loc[result['label'] == -1]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_columns = ['StartPoint.X', 'StartPoint.Y', 'StartPoint.Z',\n",
    "             'EndPoint.X', 'EndPoint.Y', 'EndPoint.Z']\n",
    "y_columns = ['XLine1Point.X', 'XLine1Point.Y','XLine1Point.Z', \n",
    "    'XLine2Point.X', 'XLine2Point.Y', 'XLine2Point.Z']\n",
    "join_index = ['FileId','label']\n",
    "groupping_columns = [ 'ClassName', 'FileId','label']\n",
    "dataset_columns = x_columns + y_columns + groupping_columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing import expand_columns\n",
    "\n",
    "labeled_data = pd.DataFrame(data)\n",
    "labeled_data['label'] = -1\n",
    "\n",
    "i = 0\n",
    "for file_id, _x1 in file_points.items():\n",
    "    if len(_x1) > 20000:\n",
    "        print(file_id, \" len>20000\")\n",
    "        continue\n",
    "    #print(df.groupby(['FileId', 'label']).count())\n",
    "    #df = df.join(data_indexes, on=['id'], how='left', rsuffix='_r')\n",
    "    labeled_data = AddLabels2Dataset(\n",
    "        points_of_one_file=_x1, \n",
    "        df_to_export_labels=labeled_data, \n",
    "        fileid_to_print=file_id,\n",
    "        epses=[0.03]\n",
    "    )\n",
    "    \n",
    "    i+=1\n",
    "    if i%10=0:\n",
    "        df = expand_columns(pd.DataFrame(labeled_data), column_names)\n",
    "        df = df[dataset_columns]\n",
    "        df.to_pickle('test_dataset_cluster_labeled.pickle')      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total data length:\", len(labeled_data))\n",
    "print(\"unlabeled data length:\",len(labeled_data.loc[labeled_data['label'] == -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(labeled_data)\n",
    "df = expand_columns(df, column_names)\n",
    "\n",
    "df = df[dataset_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df[\"label\"] == -1].index).groupby(['FileId']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('test_dataset_cluster_labeled.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test classes to work with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, SubsetRandomSampler\n",
    "from dataset import EntityDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_pickle('test_dataset_cluster_labeled.pickle')\n",
    "\n",
    "for x,y in EntityDataset(d)[:22]:\n",
    "    print(x.shape,y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import DwgDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwg_dataset = DwgDataset('test_dataset_cluster_labeled.pickle', batch_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwg_dataset.entities.data_frame.groupby(['FileId']).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import clear_output\n",
    "from plot_graphics import generate_file\n",
    "    \n",
    "g = dwg_dataset.entities.data_frame.groupby(['FileId', 'label'])\n",
    "for kkk in list(g.groups.keys()):\n",
    "    if kkk[0] in ['006f290c-7280-491b-b7d7-971ed82de1a5', '006f290c-7280-491b-b7d7-971ed82de1a5']:\n",
    "        chunk = g.get_group(kkk)\n",
    "        dr = generate_file(chunk, save_file=False, verbose=False)\n",
    "        display(dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_graphics import draw_sample\n",
    "\n",
    "j=0\n",
    "for (x, y) in iter(dwg_dataset.train_loader):\n",
    "    # print(x,y)\n",
    "    for i in range(len(x)):\n",
    "        # print(x[i].shape, y[i])\n",
    "        display(draw_sample(x[i],y[i]))\n",
    "        j+=1\n",
    "    if j>3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
