{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "object-detection-with-yolo-v5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zog_lDxCd0dg"
      },
      "source": [
        "# Object Detection on a Custom Dataset using YOLO v5\n",
        "\n",
        "> TL;DR Learn how to build a custom dataset for YOLO v5 (darknet compatible) and use it to fine-tune a large object detection model. The model will be ready for real-time object detection on mobile devices.\n",
        "\n",
        "In this tutorial, you'll learn how to fine-tune a pre-trained YOLO v5 model for detecting and classifying clothing items from images.\n",
        "\n",
        "- [Read the tutorial](https://www.curiousily.com/posts/object-detection-on-custom-dataset-with-yolo-v5-using-pytorch-and-python/)\n",
        "- [Run the notebook in your browser (Google Colab)](https://colab.research.google.com/drive/1e4zvS6LyhOAayEDh3bz8MXFTJcVFSvZX?usp=sharing)\n",
        "- [Read the `Getting Things Done with Pytorch` book](https://github.com/curiousily/Getting-Things-Done-with-Pytorch)\n",
        "\n",
        "Here's what we'll go over:\n",
        "\n",
        "- Install required libraries\n",
        "- Build a custom dataset in YOLO/darknet format\n",
        "- Learn about YOLO model family history\n",
        "- Fine-tune the largest YOLO v5 model\n",
        "- Evaluate the model\n",
        "- Look at some predictions\n",
        "\n",
        "How good our final model is going to be?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6havkIS3fpWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69cf7d91-cf33-4811-e999-08f9bb86fa5a"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jul  6 07:38:21 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv9D2zLftM0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46b86446-64ed-4a1e-a8bd-b2126605fef3"
      },
      "source": [
        "!sudo apt-get install tree -qq > /dev/null"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoYi0vbKeIyw"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "Let's start by installing some required libraries by the YOLOv5 project:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2UQU5T0hJcL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "956554f0-0dca-442b-9de4-c4d324e93d5b"
      },
      "source": [
        "!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install numpy==1.17\n",
        "!pip install PyYAML==5.3.1\n",
        "!pip install git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 26kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 503kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.5.1+cu101) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.6.1+cu101) (7.1.2)\n",
            "\u001b[31mERROR: torchtext 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.5.1+cu101 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n",
            "Collecting numpy==1.17\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/4b/55cfbfd3e5e85016eeef9f21c0ec809d978706a0d60b62cc28aeec8c792f/numpy-1.17.0-cp37-cp37m-manylinux1_x86_64.whl (20.3MB)\n",
            "\u001b[K     |████████████████████████████████| 20.3MB 42.2MB/s \n",
            "\u001b[31mERROR: torchtext 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.5.1+cu101 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement numpy~=1.19.2, but you'll have numpy 1.17.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement numpy>=1.18.5, but you'll have numpy 1.17.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "Successfully installed numpy-1.17.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting PyYAML==5.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 15.4MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 21.2MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 23.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 40kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 61kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 71kB 11.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 81kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 92kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 102kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 112kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 122kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 133kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 143kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 153kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 163kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 174kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 184kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 194kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 204kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 215kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 225kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 235kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 245kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 256kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 266kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 13.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyYAML\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44636 sha256=4ee474e31139ffbe33dacac3ccd8ad30c0eb657efe1cd388d195c6cf4281835f\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "Successfully built PyYAML\n",
            "Installing collected packages: PyYAML\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.3.1\n",
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-zd72p76c\n",
            "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-zd72p76c\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (57.0.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (0.29.23)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.17.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools==2.0) (1.15.0)\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp37-cp37m-linux_x86_64.whl size=265038 sha256=4d8e6258f443952069909a78581cd09c17ed76ca285efc5fd8bbe8748bf6326d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8imx5gtq/wheels/90/51/41/646daf401c3bc408ff10de34ec76587a9b3ebfac8d21ca5c3a\n",
            "Successfully built pycocotools\n",
            "Installing collected packages: pycocotools\n",
            "  Found existing installation: pycocotools 2.0.2\n",
            "    Uninstalling pycocotools-2.0.2:\n",
            "      Successfully uninstalled pycocotools-2.0.2\n",
            "Successfully installed pycocotools-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utBazSS6h8tQ"
      },
      "source": [
        "We'll also install [Apex by NVIDIA](https://nvidia.github.io/apex/) to speed up the training of our model (this step is optional):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NgVhQ3yxbEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "601c3229-f604-4b58-c8b8-5bcb057261ec"
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex && cd apex && pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" . --user && cd .. && rm -rf apex"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 8054, done.\u001b[K\n",
            "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
            "remote: Compressing objects: 100% (92/92), done.\u001b[K\n",
            "remote: Total 8054 (delta 68), reused 97 (delta 44), pack-reused 7913\u001b[K\n",
            "Receiving objects: 100% (8054/8054), 14.11 MiB | 19.44 MiB/s, done.\n",
            "Resolving deltas: 100% (5469/5469), done.\n",
            "/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-zsubesdv\n",
            "Created temporary directory: /tmp/pip-req-tracker-lgu6q9wu\n",
            "Created requirements tracker '/tmp/pip-req-tracker-lgu6q9wu'\n",
            "Created temporary directory: /tmp/pip-install-j1ffsb6z\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-s_d29xar\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-lgu6q9wu'\n",
            "    Running setup.py (path:/tmp/pip-req-build-s_d29xar/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.5.1+cu101\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-s_d29xar/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-s_d29xar/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-s_d29xar/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-s_d29xar/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-s_d29xar/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file '/tmp/pip-req-build-s_d29xar/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-s_d29xar/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-s_d29xar has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-lgu6q9wu'\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-record-4822vs1o\n",
            "    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-s_d29xar/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-s_d29xar/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-4822vs1o/install-record.txt --single-version-externally-managed --compile --user --prefix=\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.5.1+cu101\n",
            "\n",
            "\n",
            "    /tmp/pip-req-build-s_d29xar/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "    Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "    Cuda compilation tools, release 11.0, V11.0.221\n",
            "    Build cuda_11.0_bu.TC445_37.28845127_0\n",
            "    from /usr/local/cuda/bin\n",
            "\n",
            "    Traceback (most recent call last):\n",
            "      File \"<string>\", line 1, in <module>\n",
            "      File \"/tmp/pip-req-build-s_d29xar/setup.py\", line 171, in <module>\n",
            "        check_cuda_torch_binary_vs_bare_metal(torch.utils.cpp_extension.CUDA_HOME)\n",
            "      File \"/tmp/pip-req-build-s_d29xar/setup.py\", line 106, in check_cuda_torch_binary_vs_bare_metal\n",
            "        \"https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  \"\n",
            "    RuntimeError: Cuda extensions are being compiled with a version of Cuda that does not match the version used to compile Pytorch binaries.  Pytorch binaries were compiled with Cuda 10.1.\n",
            "    In some cases, a minor-version mismatch will not cause later errors:  https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  You can try commenting out this check (at your own risk).\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25herror\n",
            "Cleaning up...\n",
            "  Removing source in /tmp/pip-req-build-s_d29xar\n",
            "Removed build tracker '/tmp/pip-req-tracker-lgu6q9wu'\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-s_d29xar/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-s_d29xar/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-4822vs1o/install-record.txt --single-version-externally-managed --compile --user --prefix= Check the logs for full command output.\u001b[0m\n",
            "Exception information:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 153, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 455, in run\n",
            "    use_user_site=options.use_user_site,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/req/__init__.py\", line 62, in install_given_reqs\n",
            "    **kwargs\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/req/req_install.py\", line 888, in install\n",
            "    cwd=self.unpacked_source_directory,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/subprocess.py\", line 275, in runner\n",
            "    spinner=spinner,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/subprocess.py\", line 242, in call_subprocess\n",
            "    raise InstallationError(exc_msg)\n",
            "pip._internal.exceptions.InstallationError: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-s_d29xar/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-s_d29xar/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-4822vs1o/install-record.txt --single-version-externally-managed --compile --user --prefix= Check the logs for full command output.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yV-H5yKI5o5"
      },
      "source": [
        "## Build a dataset\n",
        "\n",
        "The dataset contains annotations for clothing items - bounding boxes around shirts, tops, jackets, sunglasses. The dataset is from [DataTurks](https://dataturks.com/) and is on [Kaggle](https://www.kaggle.com/dataturks/clothing-item-detection-for-ecommerce)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv02awtqMSkv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f425268f-606c-4d56-dd9f-94507b0d52dc"
      },
      "source": [
        "!gdown --id 1uWdQ2kn25RSQITtBHa9_zayplm27IXNC"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1uWdQ2kn25RSQITtBHa9_zayplm27IXNC\n",
            "To: /content/clothing.json\n",
            "\r  0% 0.00/199k [00:00<?, ?B/s]\r100% 199k/199k [00:00<00:00, 31.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCTsGOXC0M4c"
      },
      "source": [
        "The dataset contains a single JSON file with URLs to all images and bounding box data.\n",
        "\n",
        "Let's import all required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoy_rgWmIQwv"
      },
      "source": [
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import json\n",
        "import urllib\n",
        "import PIL.Image as Image\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "from IPython.display import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "rcParams['figure.figsize'] = 16, 10\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-s9IiOQ7naN"
      },
      "source": [
        "Each line in the dataset file contains a JSON object. Let's create a list of all annotations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIEvOvuaOe5G"
      },
      "source": [
        "clothing = []\n",
        "with open(\"clothing.json\") as f:\n",
        "    for line in f:\n",
        "        clothing.append(json.loads(line))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HBLG1ES82u0"
      },
      "source": [
        "Here's an example annotation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhazKaoqQXWX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2894cbc-6bfd-4177-8c97-af203d5e2ea5"
      },
      "source": [
        "clothing[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'annotation': [{'imageHeight': 312,\n",
              "   'imageWidth': 147,\n",
              "   'label': ['Tops'],\n",
              "   'notes': '',\n",
              "   'points': [{'x': 0.02040816326530612, 'y': 0.2532051282051282},\n",
              "    {'x': 0.9931972789115646, 'y': 0.8108974358974359}]}],\n",
              " 'content': 'http://com.dataturks.a96-i23.open.s3.amazonaws.com/2c9fafb063ad2b650163b00a1ead0017/4bb8fd9d-8d52-46c7-aa2a-9c18af10aed6___Data_xxl-top-4437-jolliy-original-imaekasxahykhd3t.jpeg',\n",
              " 'extras': None}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWyYCwmzE5zm"
      },
      "source": [
        "We have the labels, image dimensions, bounding box points (normalized in 0-1 range), and an URL to the image file.\n",
        "\n",
        "Do we have images with multiple annotations?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2CY21JMIH1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "b371079d-3330-4c7f-e6ad-112ce2975bb2"
      },
      "source": [
        "for c in clothing:\n",
        "  if len(c['annotation']) > 1:\n",
        "    display(c)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'annotation': [{'imageHeight': 312,\n",
              "   'imageWidth': 265,\n",
              "   'label': ['Jackets'],\n",
              "   'notes': '',\n",
              "   'points': [{'x': 0, 'y': 0.6185897435897436},\n",
              "    {'x': 0.026415094339622643, 'y': 0.6185897435897436}]},\n",
              "  {'imageHeight': 312,\n",
              "   'imageWidth': 265,\n",
              "   'label': ['Skirts'],\n",
              "   'notes': '',\n",
              "   'points': [{'x': 0.01509433962264151, 'y': 0.03205128205128205},\n",
              "    {'x': 1, 'y': 0.9839743589743589}]}],\n",
              " 'content': 'http://com.dataturks.a96-i23.open.s3.amazonaws.com/2c9fafb063ad2b650163b00a1ead0017/b3be330c-c211-45bb-b244-11aef08021c8___Data_free-sk-5108-mudrika-original-imaf4fz626pegq9f.jpeg',\n",
              " 'extras': None}"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo_LKL_6H8Nx"
      },
      "source": [
        "Just a single example. We'll need to handle it, though. \n",
        "\n",
        "Let's get all unique categories:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiKhCePRkyiL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d115c3bd-3a5f-4904-c969-0fd273eb1429"
      },
      "source": [
        "categories = []\n",
        "for c in clothing:\n",
        "  for a in c['annotation']:\n",
        "    categories.extend(a['label'])\n",
        "categories = list(set(categories))\n",
        "categories.sort()\n",
        "categories"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Jackets',\n",
              " 'Jeans',\n",
              " 'Shirts',\n",
              " 'Shoes',\n",
              " 'Skirts',\n",
              " 'Tops',\n",
              " 'Trousers',\n",
              " 'Tshirts',\n",
              " 'sunglasses']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUv66QNZJjMb"
      },
      "source": [
        "We have 9 different categories. Let's split the data into a training and validation set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i8pD6JHJexe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bb67b06-ec8b-4475-bb3a-b95fcd8a1c5a"
      },
      "source": [
        "train_clothing, val_clothing = train_test_split(clothing, test_size=0.1)\n",
        "len(train_clothing), len(val_clothing)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(453, 51)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj0c1HzklzPh"
      },
      "source": [
        "### Sample image and annotation\n",
        "\n",
        "Let's have a look at an image from the dataset. We'll start by downloading it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1HCPNXsRlCS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "e191a61f-bfb3-4205-8b01-05262e33fe65"
      },
      "source": [
        "row = train_clothing[10]\n",
        "\n",
        "img = urllib.request.urlopen(row[\"content\"])\n",
        "img = Image.open(img)\n",
        "img = img.convert('RGB')\n",
        "\n",
        "img.save(\"demo_image.jpeg\", \"JPEG\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-86f1ba8d9cd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_clothing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 641\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBVCVinxstzC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68630aa5-f4ac-4c78-9044-62423f08c303"
      },
      "source": [
        "row"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'annotation': [{'imageHeight': 312,\n",
              "   'imageWidth': 145,\n",
              "   'label': ['Tops'],\n",
              "   'notes': '',\n",
              "   'points': [{'x': 0.013793103448275862, 'y': 0.22756410256410256},\n",
              "    {'x': 1, 'y': 0.7948717948717948}]}],\n",
              " 'content': 'http://com.dataturks.a96-i23.open.s3.amazonaws.com/2c9fafb063ad2b650163b00a1ead0017/ec339ad6-6b73-406a-8971-f7ea35d47577___Data_s-top-203-red-srw-original-imaf2nfrxdzvhh3k.jpeg',\n",
              " 'extras': None}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM2H7jICq5dz"
      },
      "source": [
        "We can use OpenCV to read the image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XvOk166stT_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "6650cb0f-652c-434c-804f-7a20fed5bd01"
      },
      "source": [
        "img = cv2.cvtColor(cv2.imread(f'demo_image.jpeg'), cv2.COLOR_BGR2RGB)\n",
        "img.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-3bb086923cce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'demo_image.jpeg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2smwA6NAvRrM"
      },
      "source": [
        "Let's add the bounding box on top of the image along with the label:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2_sJMkrShko",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "ec30a08a-6519-4fc2-8a81-7f654c3f07ae"
      },
      "source": [
        "for a in row['annotation']:\n",
        "  for label in a['label']:\n",
        "\n",
        "    w = a['imageWidth']\n",
        "    h = a['imageHeight']\n",
        "\n",
        "    points = a['points']\n",
        "    p1, p2 = points\n",
        "\n",
        "    x1, y1 = p1['x'] * w, p1['y'] * h\n",
        "    x2, y2 = p2['x'] * w, p2['y'] * h\n",
        "\n",
        "    cv2.rectangle(\n",
        "      img,\n",
        "      (int(x1), int(y1)),\n",
        "      (int(x2), int(y2)),\n",
        "      color=(0, 255, 0),\n",
        "      thickness=2\n",
        "    )\n",
        "\n",
        "    ((label_width, label_height), _) = cv2.getTextSize(\n",
        "        label, \n",
        "        fontFace=cv2.FONT_HERSHEY_PLAIN,\n",
        "        fontScale=1.75, \n",
        "        thickness=2\n",
        "    )\n",
        "\n",
        "    cv2.rectangle(\n",
        "      img,\n",
        "      (int(x1), int(y1)),\n",
        "      (int(x1 + label_width + label_width * 0.05), int(y1 + label_height + label_height * 0.25)),\n",
        "      color=(0, 255, 0),\n",
        "      thickness=cv2.FILLED\n",
        "    )\n",
        "\n",
        "    cv2.putText(\n",
        "      img,\n",
        "      label,\n",
        "      org=(int(x1), int(y1 + label_height + label_height * 0.25)), # bottom left\n",
        "      fontFace=cv2.FONT_HERSHEY_PLAIN,\n",
        "      fontScale=1.75,\n",
        "      color=(255, 255, 255),\n",
        "      thickness=2\n",
        "    )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-dc45474b8d30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     cv2.rectangle(\n\u001b[0;32m---> 14\u001b[0;31m       \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m       \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'img' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfCt50iRwXE0"
      },
      "source": [
        "The point coordinates are converted back to pixels and used to draw rectangles over the image. Here's the result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff9hFIgcQ3EH"
      },
      "source": [
        "plt.imshow(img)\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gniwd8aLQKbl"
      },
      "source": [
        "### Convert to YOLO format\n",
        "\n",
        "YOLO v5 requires the dataset to be in the *darknet format*. Here's an outline of what it looks like:\n",
        "\n",
        "- One txt with labels file per image\n",
        "- One row per object\n",
        "- Each row contains: `class_index bbox_x_center bbox_y_center bbox_width bbox_height`\n",
        "- Box coordinates must be normalized between 0 and 1\n",
        "\n",
        "Let's create a helper function that builds a dataset in the correct format for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBpB0kb0ZKzh"
      },
      "source": [
        "def create_dataset(clothing, categories, dataset_type):\n",
        "\n",
        "  images_path = Path(f\"clothing/images/{dataset_type}\")\n",
        "  images_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  labels_path = Path(f\"clothing/labels/{dataset_type}\")\n",
        "  labels_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  for img_id, row in enumerate(tqdm(clothing)):\n",
        "\n",
        "    image_name = f\"{img_id}.jpeg\"\n",
        "\n",
        "    img = urllib.request.urlopen(row[\"content\"])\n",
        "    img = Image.open(img)\n",
        "    img = img.convert(\"RGB\")\n",
        "\n",
        "    img.save(str(images_path / image_name), \"JPEG\")\n",
        "\n",
        "    label_name = f\"{img_id}.txt\"\n",
        "\n",
        "    with (labels_path / label_name).open(mode=\"w\") as label_file:\n",
        "\n",
        "      for a in row['annotation']:\n",
        "\n",
        "        for label in a['label']:\n",
        "\n",
        "          category_idx = categories.index(label)\n",
        "\n",
        "          points = a['points']\n",
        "          p1, p2 = points\n",
        "\n",
        "          x1, y1 = p1['x'], p1['y']\n",
        "          x2, y2 = p2['x'], p2['y']\n",
        "\n",
        "          bbox_width = x2 - x1\n",
        "          bbox_height = y2 - y1\n",
        "\n",
        "          label_file.write(\n",
        "            f\"{category_idx} {x1 + bbox_width / 2} {y1 + bbox_height / 2} {bbox_width} {bbox_height}\\n\"\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCtfTtYcZI3o"
      },
      "source": [
        "create_dataset(train_clothing, categories, 'train')\n",
        "create_dataset(val_clothing, categories, 'val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDUVWaNxY1B_"
      },
      "source": [
        "!tree clothing -L 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQz3fLolZS9C"
      },
      "source": [
        "!cat clothing/labels/train/0.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZjfcoT3I8y-"
      },
      "source": [
        "## Fine-tuning YOLO v5\n",
        "\n",
        "The YOLO abbreviation stands for You Only Look Once. YOLO models are one stage object detectors. \n",
        "\n",
        "YOLO models are very light and fast. They are [not the most accurate object detections around](https://paperswithcode.com/sota/object-detection-on-coco), though. Ultimately, those models are the choice of many (if not all) practitioners interested in [real-time object detection (FPS >30)](https://paperswithcode.com/sota/real-time-object-detection-on-coco).\n",
        "\n",
        "### Contreversy\n",
        "\n",
        "Joseph Redmon introduced YOLO v1 in the 2016 paper [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/pdf/1506.02640.pdf). The implementation uses the [Darknet Neural Networks library](https://pjreddie.com/darknet/).\n",
        "\n",
        "He also co-authored the YOLO v2 paper in 2017  [YOLO9000: Better, Faster, Stronger](https://arxiv.org/pdf/1612.08242.pdf). A significant improvement over the first iteration with much better localization of objects.\n",
        "\n",
        "The final iteration, from the original author, was published in the 2018 paper [YOLOv3: An Incremental Improvement](https://arxiv.org/pdf/1804.02767.pdf).\n",
        "\n",
        "Then things got a bit wacky. Alexey Bochkovskiy published [YOLOv4: Optimal Speed and Accuracy of Object Detection](https://arxiv.org/abs/2004.10934) on April 23, 2020. The project has [an open-source repository on GitHub](https://github.com/AlexeyAB/darknet).\n",
        "\n",
        "YOLO v5 got open-sourced on [May 30, 2020](https://github.com/ultralytics/yolov5/commit/1e84a23f38fad9e52b59101e9f1246d93066ed1e) by [Glenn Jocher](https://github.com/glenn-jocher) from ultralytics. There is no published paper, but [the complete project is on GitHub](https://github.com/ultralytics/yolov5).\n",
        "\n",
        "The community at Hacker News got into a [heated debate about the project naming](https://news.ycombinator.com/item?id=23478151). Even the guys at Roboflow wrote [Responding to the Controversy about YOLOv5](https://blog.roboflow.ai/yolov4-versus-yolov5/) article about it. They also did a great comparison between YOLO v4 and v5.\n",
        "\n",
        "My opinion? As long as you put out your work for the whole world to use/see - I don't give a flying fuck. I am not going to comment on points/arguments that are obvious.\n",
        "\n",
        "### YOLO v5 project setup\n",
        "\n",
        "YOLO v5 uses PyTorch, but everything is abstracted away. You need the project itself (along with the required dependencies).\n",
        "\n",
        "Let's start by cloning the GitHub repo and checking out a specific commit (to ensure reproducibility):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbrsPZV4Wza1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e4c8c61-2a5d-40bd-97aa-db9bdb38a6ff"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "!git checkout ec72eea62bf5bb86b0272f2e65e413957533507f"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 7771, done.\u001b[K\n",
            "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 7771 (delta 46), reused 59 (delta 36), pack-reused 7688\u001b[K\n",
            "Receiving objects: 100% (7771/7771), 9.37 MiB | 24.06 MiB/s, done.\n",
            "Resolving deltas: 100% (5342/5342), done.\n",
            "/content/yolov5\n",
            "Note: checking out 'ec72eea62bf5bb86b0272f2e65e413957533507f'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at ec72eea Merge remote-tracking branch 'origin/master'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osaSm-_JVuNN"
      },
      "source": [
        "We need two configuration files. One for the dataset and one for the model we're going to use. Let's download them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EidVMrZgEteg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f051a40-2d29-4a5b-e99e-bb3fc797a0a5"
      },
      "source": [
        "!gdown --id 1ZycPS5Ft_0vlfgHnLsfvZPhcH6qOAqBO -O data/clothing.yaml\n",
        "!gdown --id 1czESPsKbOWZF7_PkCcvRfTiUUJfpx12i -O models/yolov5x.yaml"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ZycPS5Ft_0vlfgHnLsfvZPhcH6qOAqBO\n",
            "To: /content/yolov5/data/clothing.yaml\n",
            "100% 172/172 [00:00<00:00, 209kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1czESPsKbOWZF7_PkCcvRfTiUUJfpx12i\n",
            "To: /content/yolov5/models/yolov5x.yaml\n",
            "100% 1.58k/1.58k [00:00<00:00, 2.82MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8DhmHUQYl7u"
      },
      "source": [
        "The model config changes the number of classes to 9 (equal to the ones in our dataset). The dataset config `clothing.yaml` is a bit more complex:\n",
        "\n",
        "```yaml\n",
        "train: ../clothing/images/train/\n",
        "val: ../clothing/images/val/\n",
        "\n",
        "nc: 9\n",
        "\n",
        "names: ['Jackets', 'Jeans', 'Shirts', 'Shoes', 'Skirts', 'Tops', 'Trousers', 'Tshirts', 'sunglasses']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mrRD_1WgoKz"
      },
      "source": [
        "This file specifies the paths to the training and validation sets. It also gives the number of classes and their names (you should order those correctly).\n",
        "\n",
        "### Training\n",
        "\n",
        "Fine-tuning an existing model is very easy. We'll use the largest model **YOLOv5x** (89M parameters), which is also the most accurate.\n",
        "\n",
        "In our case, we don't really care about speed. We just want the best accuracy you can get. The checkpoint you're going to use for a different problem(s) is contextually specific. [Take a look at the overview of the pre-trained checkpoints](https://github.com/ultralytics/yolov5/blob/f9ae460eeccd30bdc43a89a37f74b9cc7b93d52f/README.md#pretrained-checkpoints).\n",
        "\n",
        "To train a model on a custom dataset, we'll call the `train.py` script. We'll pass a couple of parameters:\n",
        "\n",
        "- img 640 - resize the images to 640x640 pixels\n",
        "- batch 4 - 4 images per batch\n",
        "- epochs 30 - train for 30 epochs\n",
        "- data ./data/clothing.yaml - path to dataset config\n",
        "- cfg ./models/yolov5x.yaml - model config\n",
        "- weights yolov5x.pt - use pre-trained weights from the YOLOv5x model\n",
        "- name yolov5x_clothing - name of our model\n",
        "- cache - cache dataset images for faster training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21XV3yUgi_1_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79fcea45-8bd6-4302-cf5e-28fa51207815"
      },
      "source": [
        "!python train.py --img 640 --batch 4 --epochs 30 \\\n",
        "  --data ./data/clothing.yaml --cfg ./models/yolov5x.yaml --weights yolov5x.pt \\\n",
        "  --name yolov5x_clothing --cache"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apex recommended for faster mixed precision training: https://github.com/NVIDIA/apex\n",
            "{'lr0': 0.01, 'momentum': 0.937, 'weight_decay': 0.0005, 'giou': 0.05, 'cls': 0.58, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0, 'iou_t': 0.2, 'anchor_t': 4.0, 'fl_gamma': 0.0, 'hsv_h': 0.014, 'hsv_s': 0.68, 'hsv_v': 0.36, 'degrees': 0.0, 'translate': 0.0, 'scale': 0.5, 'shear': 0.0}\n",
            "Namespace(adam=False, batch_size=4, bucket='', cache_images=True, cfg='./models/yolov5x.yaml', data='./data/clothing.yaml', device='', epochs=30, evolve=False, img_size=[640], multi_scale=False, name='yolov5x_clothing', noautoanchor=False, nosave=False, notest=False, rect=False, resume=False, single_cls=False, weights='yolov5x.pt')\n",
            "Using CUDA device0 _CudaDeviceProperties(name='Tesla K80', total_memory=11441MB)\n",
            "\n",
            "2021-07-06 07:50:08.474945: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Start Tensorboard with \"tensorboard --logdir=runs\", view at http://localhost:6006/\n",
            "\n",
            "              from  n    params  module                                  arguments                     \n",
            "  0             -1  1      8800  models.common.Focus                     [3, 80, 3]                    \n",
            "  1             -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               \n",
            "  2             -1  1    315680  models.common.BottleneckCSP             [160, 160, 4]                 \n",
            "  3             -1  1    461440  models.common.Conv                      [160, 320, 3, 2]              \n",
            "  4             -1  1   3311680  models.common.BottleneckCSP             [320, 320, 12]                \n",
            "  5             -1  1   1844480  models.common.Conv                      [320, 640, 3, 2]              \n",
            "  6             -1  1  13228160  models.common.BottleneckCSP             [640, 640, 12]                \n",
            "  7             -1  1   7375360  models.common.Conv                      [640, 1280, 3, 2]             \n",
            "  8             -1  1   4099840  models.common.SPP                       [1280, 1280, [5, 9, 13]]      \n",
            "  9             -1  1  20087040  models.common.BottleneckCSP             [1280, 1280, 4, False]        \n",
            " 10             -1  1    820480  models.common.Conv                      [1280, 640, 1, 1]             \n",
            " 11             -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12        [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13             -1  1   5435520  models.common.BottleneckCSP             [1280, 640, 4, False]         \n",
            " 14             -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \n",
            " 15             -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16        [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17             -1  1   1360960  models.common.BottleneckCSP             [640, 320, 4, False]          \n",
            " 18             -1  1     13482  torch.nn.modules.conv.Conv2d            [320, 42, 1, 1]               \n",
            " 19             -2  1    922240  models.common.Conv                      [320, 320, 3, 2]              \n",
            " 20       [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 21             -1  1   5025920  models.common.BottleneckCSP             [640, 640, 4, False]          \n",
            " 22             -1  1     26922  torch.nn.modules.conv.Conv2d            [640, 42, 1, 1]               \n",
            " 23             -2  1   3687680  models.common.Conv                      [640, 640, 3, 2]              \n",
            " 24       [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 25             -1  1  20087040  models.common.BottleneckCSP             [1280, 1280, 4, False]        \n",
            " 26             -1  1     53802  torch.nn.modules.conv.Conv2d            [1280, 42, 1, 1]              \n",
            " 27   [-1, 22, 18]  1         0  models.yolo.Detect                      [9, [[116, 90, 156, 198, 373, 326], [30, 61, 62, 45, 59, 119], [10, 13, 16, 30, 33, 23]]]\n",
            "Model Summary: 407 layers, 8.84875e+07 parameters, 8.84875e+07 gradients\n",
            "\n",
            "Optimizer groups: 134 .bias, 142 conv.weight, 131 other\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   141    0   141    0     0    909      0 --:--:-- --:--:-- --:--:--   903\n",
            "Downloading https://drive.google.com/uc?export=download&id=1mM8aZJlWTxOg7BZJvNUMrTnA2AbeCVzS as yolov5x.pt... Done (0.5s)\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   196  100   196    0     0   5600      0 --:--:-- --:--:-- --:--:--  5600\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 404, in <module>\n",
            "    train(hyp)\n",
            "  File \"train.py\", line 109, in train\n",
            "    google_utils.attempt_download(weights)\n",
            "  File \"/content/yolov5/utils/google_utils.py\", line 36, in attempt_download\n",
            "    raise Exception(msg)\n",
            "Exception: yolov5x.pt missing, try downloading from https://drive.google.com/drive/folders/1Drs_Aiu7xx6S-ix95f9kNsA6ueKRpN2J\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qftF9rOg4Hg"
      },
      "source": [
        "The training took around 30 minutes on Tesla P100. The best model checkpoint is saved to `weights/best_yolov5x_clothing.pt`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwYB9OIJk__n"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "The project includes a great utility function `plot_results()` that allows you to evaluate your model performance on the last training run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvQJa9kuoiTD"
      },
      "source": [
        "from utils.utils import plot_results\n",
        "\n",
        "plot_results();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGrHJKGRq0zF"
      },
      "source": [
        "Looks like the mean average precision (mAP) is getting better throughout the training. The model might benefit from more training, but it is good enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZPi_kw4i6CT"
      },
      "source": [
        "## Making predictions\n",
        "\n",
        "Let's pick 50 images from the validation set and move them to `inference/images` to see how our model does on those:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqctHA417cZc"
      },
      "source": [
        "!find ../clothing/images/val/ -maxdepth 1 -type f | head -50 | xargs cp -t \"./inference/images/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05DE9VfuvI07"
      },
      "source": [
        "We'll use the `detect.py` script to run our model on the images. Here are the parameters we're using:\n",
        "\n",
        "- weights weights/best_yolov5x_clothing.pt - checkpoint of the model\n",
        "- img 640 - resize the images to 640x640 px\n",
        "- conf 0.4 - take into account predictions with confidence of 0.4 or higher\n",
        "- source ./inference/images/ - path to the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUNjsslF7z64"
      },
      "source": [
        "!python detect.py --weights weights/best_yolov5x_clothing.pt \\\n",
        "  --img 640 --conf 0.4 --source ./inference/images/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8OdN5xnwid5"
      },
      "source": [
        "We'll write a helper function to show the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh3Z-4wEuEM6"
      },
      "source": [
        "def load_image(img_path: Path, resize=True):\n",
        "  img = cv2.cvtColor(cv2.imread(str(img_path)), cv2.COLOR_BGR2RGB)\n",
        "  img = cv2.resize(img, (128, 256), interpolation = cv2.INTER_AREA)\n",
        "  return img\n",
        "\n",
        "def show_grid(image_paths):\n",
        "  images = [load_image(img) for img in image_paths]\n",
        "  images = torch.as_tensor(images)\n",
        "  images = images.permute(0, 3, 1, 2)\n",
        "  grid_img = torchvision.utils.make_grid(images, nrow=11)\n",
        "  plt.figure(figsize=(24, 12))\n",
        "  plt.imshow(grid_img.permute(1, 2, 0))\n",
        "  plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL3EA5OUwrhD"
      },
      "source": [
        "Here are some of the images along with the detected clothing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPxPJbT1wsPo"
      },
      "source": [
        "img_paths = list(Path(\"inference/output\").glob(\"*.jpeg\"))[:22]\n",
        "show_grid(img_paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPfFrYCFxKhJ"
      },
      "source": [
        "To be honest with you. I am really blown away with the results!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVSC7G7slCc4"
      },
      "source": [
        "## Summary\n",
        "\n",
        "You now know how to create a custom dataset and fine-tune one of the YOLO v5 models on your own. Nice!\n",
        "\n",
        "- [Read the tutorial](https://www.curiousily.com/posts/object-detection-on-custom-dataset-with-yolo-v5-using-pytorch-and-python/)\n",
        "- [Run the notebook in your browser (Google Colab)](https://colab.research.google.com/drive/1e4zvS6LyhOAayEDh3bz8MXFTJcVFSvZX?usp=sharing)\n",
        "- [Read the `Getting Things Done with Pytorch` book](https://github.com/curiousily/Getting-Things-Done-with-Pytorch)\n",
        "\n",
        "Here's what you've learned:\n",
        "\n",
        "- Install required libraries\n",
        "- Build a custom dataset in YOLO/darknet format\n",
        "- Learn about YOLO model family history\n",
        "- Fine-tune the largest YOLO v5 model\n",
        "- Evaluate the model\n",
        "- Look at some predictions\n",
        "\n",
        "How well does your model do on your dataset? Let me know in the comments below.\n",
        "\n",
        "In the next part, you'll learn how to deploy your model a mobile device."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sko8kmCJqQW2"
      },
      "source": [
        "## References\n",
        "\n",
        "- [Clothing Item Detection for E-Commerce dataset](https://www.kaggle.com/dataturks/clothing-item-detection-for-ecommerce)\n",
        "- [YOLOv5 GitHub](https://github.com/ultralytics/yolov5)\n",
        "- [YOLOv5 Train on Custom Data](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data)\n",
        "- [NVIDIA Apex on GitHub](https://github.com/NVIDIA/apex)\n",
        "- [YOLOv4: Optimal Speed and Accuracy of Object Detection](https://arxiv.org/pdf/2004.10934.pdf)"
      ]
    }
  ]
}