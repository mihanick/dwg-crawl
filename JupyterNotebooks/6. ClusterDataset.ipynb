{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster drawing to create samples from it\n",
    "Previous attemps to learn on entire drawing was unsuccessfull, so I decided to move to new approach.\n",
    "I.e. i will try to create a cluster of lines from the drawing that will be a learning sample x.\n",
    "Than we will calculate some distance from cluster to dimensions on the drawing to attribute each dimension with the cluster. That attribution will be our Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with basic example of unsupervised clustering with unknown number of clusters\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "\n",
    "# from processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files count 322\n",
      "data length 74513\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient('mongodb://192.168.1.49:27017')\n",
    "db = client.geometry\n",
    "\n",
    "fileids  = db.files.find({'Valid':True}).distinct('FileId')\n",
    "print('files count', len(fileids))\n",
    "\n",
    "# all the data will be too much for now, so we will take say 42 files:\n",
    "fileids = fileids[36:57]\n",
    "\n",
    "query = {\n",
    "    'ClassName':\n",
    "    {\n",
    "        '$in':[\n",
    "            'AcDbLine',\n",
    "            #'AcDbPolyline',\n",
    "            'AcDbText',\n",
    "            'AcDbRotatedDimension'\n",
    "        ]\n",
    "    },\n",
    "    'FileId':{\n",
    "        '$in':fileids\n",
    "    }\n",
    "}\n",
    "\n",
    "data = pd.DataFrame(list(db.objects.find(query)))\n",
    "print('data length', len(data))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What am I going to do here:\n",
    "We group queried data by FileId\n",
    "We get points from each file's primitives (including dimensions). We save each object in dictionary with its points.\n",
    "We cluster points (get clusters from each file)\n",
    "We get objects in each cluster \n",
    "So a single sample is single a X: tensor shape(1..nobjects) with features of objects, Y: tensor shape ndims(could be 0) of with features of dimensions \n",
    "\n",
    "Than in inference we're going to start command that will take entire drawing's entities, cluster them and predict dimensions for each cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data coordinates to columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of points:  133226\n"
     ]
    }
   ],
   "source": [
    "from processing import Col2Numpy\n",
    "\n",
    "groupped = data.groupby(['FileId'])\n",
    "column_names = ['StartPoint', 'EndPoint', 'XLine1Point', 'XLine2Point']\n",
    "\n",
    "# Whole point here is that we're keeping index from the original dataset in file_points\n",
    "\n",
    "file_points = {} #dic[FileId]list_Of_Points\n",
    "# all file ids\n",
    "file_ids = list(groupped.groups.keys())\n",
    "for file_id in file_ids:\n",
    "    pnts = Col2Numpy(groupped.get_group(file_id), column_names)\n",
    "    # print(pnts)\n",
    "    file_points[file_id]=pnts\n",
    "\n",
    "num_points = 0\n",
    "for k in file_points:\n",
    "    num_points += len(file_points[k])\n",
    "print('Total number of points: ', num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement and check clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clusterize(x, eps = 0.2):\n",
    "    '''\n",
    "    The very basic clustering with unknown number of clusters\n",
    "    https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py\n",
    "    '''\n",
    "    \n",
    "    # algorithm='kd_tree'\n",
    "    db = DBSCAN(eps=eps, min_samples=6, n_jobs=-1).fit(x)\n",
    "    labels = db.labels_\n",
    "    \n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "    \n",
    "    return labels, n_clusters, n_noise, db.core_sample_indices_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and display clustering result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_graphics import draw_set\n",
    "from processing import scale_ds\n",
    "\n",
    "\n",
    "def AddLabels2Dataset(points_of_one_file, df_to_export_labels, fileid_to_print=\"\", epses=0.2):\n",
    "    result = df_to_export_labels\n",
    "    \n",
    "    # split indexes from coordinates\n",
    "    x1, data_indexes = np.split(points_of_one_file,[3], axis = 1)\n",
    "    data_indexes = data_indexes.astype('int')\n",
    "    \n",
    "    # print(x1)\n",
    "    # scale dataframe in order to clusterize it properly\n",
    "    x1, scl = scale_ds(x1)\n",
    "    \n",
    "    # for now we will magically assign eps on base of scale\n",
    "    # epses = 15*scl\n",
    "    \n",
    "    # epses could be a number or list of numbers\n",
    "    # I use list of numbers in order\n",
    "    # to visually assess clustering\n",
    "    ep = epses\n",
    "    if type(epses) != list:\n",
    "        ep=[epses]\n",
    "        \n",
    "    for eps in ep:\n",
    "        # perform clustering of points to calculate labels\n",
    "        \n",
    "        labels, n_clusters, n_noise, core_indices = Clusterize(x1, eps=eps)\n",
    "\n",
    "        # print(len(x1), len(labels))\n",
    "        # draw_set(x1, labels, core_indices)  \n",
    "\n",
    "        print(\"fileid:{} points:{} clusters:{} noise pnts:{} eps:{:0.3f} scale:{:0.6f}\".format(\n",
    "              fileid_to_print,\n",
    "              x1.shape[0], \n",
    "              n_clusters,\n",
    "              n_noise, \n",
    "              eps,\n",
    "              scl))\n",
    "    \n",
    "    data_indexes['label'] = labels\n",
    "\n",
    "    # https://stackoverflow.com/questions/22918212/fastest-way-to-drop-duplicated-index-in-a-pandas-dataframe\n",
    "    # data_indexes = data_indexes.drop_duplicates('id')\n",
    "    data_indexes = data_indexes.groupby(data_indexes.index).first()\n",
    "    \n",
    "    for i in range(len(data_indexes.index)):\n",
    "        indx = data_indexes.index[i]\n",
    "        # https://stackoverflow.com/questions/13021654/get-column-index-from-column-name-in-python-pandas\n",
    "        result.iloc[indx, result.columns.get_loc('label')] = data_indexes.iloc[i, data_indexes.columns.get_loc('label')]    \n",
    "    print(len(result.loc[result['label'] == -1]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileid:0b44dda8-5bbf-4eee-824e-1d52e2dd03f2 points:590 clusters:9 noise pnts:72 eps:0.030 scale:0.001698\n",
      "74255\n",
      "fileid:0bf244b9-3454-471e-a607-11e47d86bf81 points:1130 clusters:26 noise pnts:92 eps:0.030 scale:0.001199\n",
      "73736\n",
      "fileid:0bf33e32-6be0-4b35-972d-07f1354719fd points:314 clusters:5 noise pnts:36 eps:0.030 scale:0.001698\n",
      "73593\n",
      "fileid:0c0325d6-6c00-46e8-99fe-3527a6115407 points:156 clusters:10 noise pnts:46 eps:0.030 scale:0.002411\n",
      "73540\n",
      "fileid:0c562f37-0a6b-420f-958d-efae5a8cd876 points:1542 clusters:29 noise pnts:27 eps:0.030 scale:0.025425\n",
      "72777\n",
      "fileid:0c9ef0a4-3051-40db-9675-099660c9c828 points:58 clusters:1 noise pnts:49 eps:0.030 scale:0.000031\n",
      "72774\n",
      "fileid:0cecdcdd-c843-4f0e-b76d-c3f1a16bdbaf points:1622 clusters:13 noise pnts:61 eps:0.030 scale:0.001512\n",
      "71989\n",
      "fileid:0d61a0d5-5193-44f3-ae42-06dbf9227e04 points:19888 clusters:4 noise pnts:0 eps:0.030 scale:0.000001\n",
      "62045\n",
      "fileid:0d8843aa-142f-478c-a1a8-d6513cb75130 points:3882 clusters:10 noise pnts:20 eps:0.030 scale:0.001029\n",
      "60113\n"
     ]
    }
   ],
   "source": [
    "labeled_data = pd.DataFrame(data)\n",
    "labeled_data['label'] = -1\n",
    "\n",
    "i = 0\n",
    "for file_id, _x1 in file_points.items():\n",
    "    #print(df.groupby(['FileId', 'label']).count())\n",
    "    #df = df.join(data_indexes, on=['id'], how='left', rsuffix='_r')\n",
    "    labeled_data = AddLabels2Dataset(\n",
    "        points_of_one_file=_x1, \n",
    "        df_to_export_labels=labeled_data, \n",
    "        fileid_to_print=file_id,\n",
    "        epses=[0.03]\n",
    "    )\n",
    "    \n",
    "    i+=1\n",
    "    if i>3:\n",
    "        #debug break\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total data length:\", len(labeled_data))\n",
    "print(\"unlabeled data length:\",len(labeled_data.loc[labeled_data['label'] == -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing import expand_columns\n",
    "\n",
    "df = pd.DataFrame(labeled_data)\n",
    "df = expand_columns(df, column_names)\n",
    "x_columns = ['StartPoint.X', 'StartPoint.Y', 'StartPoint.Z',\n",
    "             'EndPoint.X', 'EndPoint.Y', 'EndPoint.Z']\n",
    "y_columns = ['XLine1Point.X', 'XLine1Point.Y','XLine1Point.Z', \n",
    "    'XLine2Point.X', 'XLine2Point.Y', 'XLine2Point.Z']\n",
    "join_index = ['FileId','label']\n",
    "groupping_columns = [ 'ClassName', 'FileId','label']\n",
    "dataset_columns = x_columns + y_columns + groupping_columns \n",
    "\n",
    "df = df[dataset_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df[\"label\"] == -1].index).groupby(['FileId']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('test_dataset_cluster_labeled.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test classes to work with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, SubsetRandomSampler\n",
    "from dataset import EntityDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_pickle('test_dataset_cluster_labeled.pickle')\n",
    "\n",
    "for x,y in EntityDataset(d)[:2]:\n",
    "    print(x.shape,y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import DwgDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwg_dataset = DwgDataset('test_dataset_cluster_labeled.pickle', batch_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwg_dataset.entities.data_frame.groupby(['FileId']).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import clear_output\n",
    "from plot_graphics import generate_file\n",
    "    \n",
    "g = dwg_dataset.entities.data_frame.groupby(['FileId', 'label'])\n",
    "for kkk in list(g.groups.keys()):\n",
    "    if kkk[0] in ['006f290c-7280-491b-b7d7-971ed82de1a5', '006f290c-7280-491b-b7d7-971ed82de1a5']:\n",
    "        chunk = g.get_group(kkk)\n",
    "        dr = generate_file(chunk, save_file=False, verbose=False)\n",
    "        display(dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_graphics import draw_sample\n",
    "\n",
    "j=0\n",
    "for (x, y) in iter(dwg_dataset.train_loader):\n",
    "    # print(x,y)\n",
    "    for i in range(len(x)):\n",
    "        # print(x[i].shape, y[i])\n",
    "        display(draw_sample(x[i],y[i]))\n",
    "        j+=1\n",
    "    if j>3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
